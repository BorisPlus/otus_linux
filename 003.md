#  Файловые системы и LVM 

__Домашнее задание__:

Работа с LVM. На имеющемся образе /dev/mapper/VolGroup00-LogVol00 38G 738M 37G 2% /
- уменьшить том под / до 8G 
- выделить том под /home 
- выделить том под /var 
- /var - сделать в mirror 
- /home - сделать том для снэпшотов (__Примечание__: имеется ввиду иное - "сделать том для снэпшотов /home")
- прописать монтирование в fstab 
- попробовать с разными опциями и разными файловыми системами (на выбор)

- сгенерить файлы в /home/
- снять снэпшот
- удалить часть файлов
- восстановится со снэпшота
- залоггировать работу можно с помощью утилиты script

Задание со *:
- на нашей куче дисков попробовать поставить btrfs/zfs - с кешем, снэпшотами - разметить здесь каталог /opt

__Критерии оценки__:

Статус "Принято" ставится при выполнении основной части.
Задание со звездочкой выполняется по желанию.

## Результат

__Внимание__: здесь и далее команды запускаются внутри VM, то есть в терминале значится метка `[vagrant@lvm ~]$`, если не сказано иное.

Этапы:
- [уменьшить том под / до 8G](#1)
- [выделить том под /home](#2)
- [выделить том под /var](#34)
- [/var - сделать в mirror](#34)
- [/home - сделать том для снэпшотов](#56)
- [сгенерить файлы в /home/](#56)
- [снять снэпшот](#7)
- [удалить часть файлов](#8)
- [восстановится со снэпшота](#9)
- [залоггировать работу можно с помощью утилиты script](#10)

## Описание исполнения

### Предварительно

В качестве исходника выступает специально подготовленный [образ](https://gitlab.com/otus_linux/stands-03-lvm/-/blob/master/Vagrantfile). 

В ходе выполнения домашнего задания ряд пунктов решены иначе, чем в прилагаемой [методичке от преподавателя](https://cdn.otus.ru/media/private/82/4b/%D0%9F%D1%80%D0%B0%D0%BA%D1%82%D0%B8%D0%BA%D0%B0_LVM-5373-824bd9.pdf?hash=T0tmaRJ-TkV2Xv2PGsRNmQ&expires=1620543147).

На целевой:

```shell
cd ./product/prod/
vagrant up
vagrant ssh
```

На `[vagrant@lvm ~]$` проверим нахождение `VolGroup00-LogVol00`:

```shell

sudo fdisk -l | grep VolGroup00-LogVol00
    Disk /dev/mapper/VolGroup00-LogVol00: 40.2 GB, 40231763968 bytes, 78577664 sectors
    
lsblk
    NAME                    MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
    sda                       8:0    0   40G  0 disk 
    ├─sda1                    8:1    0    1M  0 part 
    ├─sda2                    8:2    0    1G  0 part /boot
    └─sda3                    8:3    0   39G  0 part 
      ├─VolGroup00-LogVol00 253:0    0 37.5G  0 lvm  /
      └─VolGroup00-LogVol01 253:1    0  1.5G  0 lvm  [SWAP]
    sdb                       8:16   0   10G  0 disk 
    sdc                       8:32   0    2G  0 disk 
    sdd                       8:48   0    1G  0 disk 
    sde                       8:64   0    1G  0 disk 
```

Итак, каталог `/` находится на `sda3` в `VolGroup00-LogVol00` и занимает 37.5G. 

Приведенный в данном описании алгоритм отличается от прилагаемого в методичке от преподавателя (в методичке предлагается LiveCD).
Для ее реализации необходим дополнительный носитель. 
Его подключение сэмулировано путем добавления в Vagrantfile дополнииельного диска в секцию с SATA (на самом деле добавление лучше произвести добавление сразу нескольких на перспективу).

__Замечание__: очень странно, но:
* при изменении значения `home` директории на иное, где создаются диски, виртуалка не разворачивается - ошибка 
* на горячую не подключить новый диск? только пересоздавать - `vagrant destroy` и `vagrant up`

```shell
:sata5 => {
    :dfile => home + '/VirtualBox VMs/sata5.vdi',
    :size => 40960,
    :port => 5
},
```
Образ таков - [Vagrantfile](./003/vm_001/Vagrantfile). Алгоритм следующий.
* Создается временный том LVM, временный раздел с XFS, 
* Переносится дамп корневого раздела на временный том.
* Производится перезагрузка с него.
* Уменьшаем исходный том до нужного размера.
* Возвращаются данные на него данные. 
* Производится перезагрузка с переделанного раздела.
* Удаляется временный том.

__Вопрос__: в методичке, прилагаемой к [домашнему заданию](https://cdn.otus.ru/media/private/82/4b/%D0%9F%D1%80%D0%B0%D0%BA%D1%82%D0%B8%D0%BA%D0%B0_LVM-5373-824bd9.pdf?hash=T0tmaRJ-TkV2Xv2PGsRNmQ&expires=1620543147), происходит загрузка с LiveCD. Мне __не понятно__, как это сделать на VM-виртуалке и с Vargant.
```shell
vagrant up
vagrant ssh
```
На `[vagrant@lvm ~]$ `:
```shell
lsblk
    NAME                    MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
    sda                       8:0    0   40G  0 disk 
    ├─sda1                    8:1    0    1M  0 part 
    ├─sda2                    8:2    0    1G  0 part /boot
    └─sda3                    8:3    0   39G  0 part 
      ├─VolGroup00-LogVol00 253:0    0 37.5G  0 lvm  /
      └─VolGroup00-LogVol01 253:1    0  1.5G  0 lvm  [SWAP]
    sdb                       8:16   0   10G  0 disk 
    sdc                       8:32   0    2G  0 disk 
    sdd                       8:48   0    1G  0 disk 
    sde                       8:64   0    1G  0 disk 
    sdf                       8:80   0   40G  0 disk <---добавили
sudo pvcreate /dev/sdf
    Physical volume "/dev/sdf" successfully created.
sudo vgcreate vg_tmp_root /dev/sdf
    Volume group "vg_tmp_root" successfully created
sudo lvcreate -n lv_tmp_root -l +100%FREE /dev/vg_tmp_root
    Logical volume "lv_tmp_root" created.
df -T / <-- Какой тип файловой системы
    Filesystem                      Type 1K-blocks   Used Available Use% Mounted on
    /dev/mapper/VolGroup00-LogVol00 xfs   39269648 792432  38477216   3% /
sudo mkfs.xfs /dev/vg_tmp_root/lv_tmp_root
    meta-data=/dev/vg_tmp_root/lv_tmp_root isize=512    agcount=4, agsize=2621184 blks
             =                       sectsz=512   attr=2, projid32bit=1
             =                       crc=1        finobt=0, sparse=0
    data     =                       bsize=4096   blocks=10484736, imaxpct=25
             =                       sunit=0      swidth=0 blks
    naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
    log      =internal log           bsize=4096   blocks=5119, version=2
             =                       sectsz=512   sunit=0 blks, lazy-count=1
    realtime =none                   extsz=4096   blocks=0, rtextents=0
sudo mount /dev/vg_tmp_root/lv_tmp_root /mnt

sudo lvdisplay
    --- Logical volume ---
    LV Path                /dev/VolGroup00/LogVol00  <-- это важно
    LV Name                LogVol00
    VG Name                VolGroup00
    LV UUID                j6b8IV-KEw3-7bTw-Oqy8-1Ud3-juFC-SJBg12
    LV Write Access        read/write
    LV Creation host, time localhost.localdomain, 2018-05-12 18:50:24 +0000
    LV Status              available
    # open                 1
    LV Size                <37.47 GiB
    Current LE             1199
    Segments               1
    Allocation             inherit
    Read ahead sectors     auto
    - currently set to     8192
    Block device           253:0
    
    --- Logical volume ---
    LV Path                /dev/VolGroup00/LogVol01
    LV Name                LogVol01
    VG Name                VolGroup00
    LV UUID                IAjIC6-ScnM-tvH6-7BTy-TN31-hd82-bgDSzd
    LV Write Access        read/write
    LV Creation host, time localhost.localdomain, 2018-05-12 18:50:25 +0000
    LV Status              available
    # open                 2
    LV Size                1.50 GiB
    Current LE             48
    Segments               1
    Allocation             inherit
    Read ahead sectors     auto
    - currently set to     8192
    Block device           253:1
    
    --- Logical volume ---
    LV Path                /dev/vg_tmp_root/lv_tmp_root
    LV Name                lv_tmp_root
    VG Name                vg_tmp_root
    LV UUID                rEkjCk-gUZY-KtFf-mHJr-AsxQ-FNqX-TRGJN6
    LV Write Access        read/write
    LV Creation host, time lvm, 2021-05-09 19:34:32 +0000
    LV Status              available
    # open                 1
    LV Size                <40.00 GiB
    Current LE             10239
    Segments               1
    Allocation             inherit
    Read ahead sectors     auto
    - currently set to     8192
    Block device           253:2
sudo yum install xfsrestore xfsdump
    ...
    Complete!
sudo xfsdump -J - /dev/VolGroup00/LogVol00 | sudo xfsrestore -J - /mnt
sudo xfsdump -J - /dev/VolGroup00/LogVol00 | sudo xfsrestore -J - /mnt
    ...
    xfsrestore: Restore Status: SUCCESS

for i in /proc/ /sys/ /dev/ /run/ /boot/; do sudo mount --bind $i /mnt/$i; done
sudo chroot /mnt/
```
Далее `[root@lvm /]`:

```shell
grub2-mkconfig -o /boot/grub2/grub.cfg
    Generating grub configuration file ...
    Found linux image: /boot/vmlinuz-3.10.0-862.2.3.el7.x86_64
    Found initrd image: /boot/initramfs-3.10.0-862.2.3.el7.x86_64.img
    done

cd /boot ; for i in `ls initramfs-*img`; do dracut -v $i `echo $i|sed "s/initramfs-//g; s/.img//g"` --force; done
    ...
    *** Creating initramfs image file '/boot/initramfs-3.10.0-862.2.3.el7.x86_64.img' done ***

cat /boot/grub2/grub.cfg | grep 'LogVol00'
    linux16 /vmlinuz-3.10.0-862.2.3.el7.x86_64 root=/dev/mapper/vg_tmp_root-lv_tmp_root ro no_timer_check console=tty0 console=ttyS0,115200n8 net.ifnames=0 biosdevname=0 elevator=noop crashkernel=auto rd.lvm.lv=VolGroup00/LogVol00 rd.lvm.lv=VolGroup00/LogVol01 rhgb quiet
cat /boot/grub2/grub.cfg | grep 'lv_tmp_root'
    linux16 /vmlinuz-3.10.0-862.2.3.el7.x86_64 root=/dev/mapper/vg_tmp_root-lv_tmp_root ro no_timer_check console=tty0 console=ttyS0,115200n8 net.ifnames=0 biosdevname=0 elevator=noop crashkernel=auto rd.lvm.lv=VolGroup00/LogVol00 rd.lvm.lv=VolGroup00/LogVol01 rhgb quiet
sed -i 's@VolGroup00/LogVol00@vg_tmp_root/lv_tmp_root@g' /boot/grub2/grub.cfg
cat /boot/grub2/grub.cfg | grep 'LogVol00'
cat /boot/grub2/grub.cfg | grep 'vg_tmp_root'
    linux16 /vmlinuz-3.10.0-862.2.3.el7.x86_64 root=/dev/mapper/vg_tmp_root-lv_tmp_root ro no_timer_check console=tty0 console=ttyS0,115200n8 net.ifnames=0 biosdevname=0 elevator=noop crashkernel=auto rd.lvm.lv=vg_tmp_root/lv_tmp_root rd.lvm.lv=VolGroup00/LogVol01 rhgb quiet 
cat /boot/grub2/grub.cfg | grep 'lv_tmp_root'
    linux16 /vmlinuz-3.10.0-862.2.3.el7.x86_64 root=/dev/mapper/vg_tmp_root-lv_tmp_root ro no_timer_check console=tty0 console=ttyS0,115200n8 net.ifnames=0 biosdevname=0 elevator=noop crashkernel=auto rd.lvm.lv=vg_tmp_root/lv_tmp_root rd.lvm.lv=VolGroup00/LogVol01 rhgb quiet 

exit
```
Далее `[root@lvm /]`:

```shell
sudo reboot <-- так
    Connection to 127.0.0.1 closed by remote host.
    Connection to 127.0.0.1 closed.
```

На целевой:

```shell
vagrant ssh
```

Далее `[root@lvm /]`:

```shell
lsblk
    NAME                      MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
    sda                         8:0    0   40G  0 disk 
    ├─sda1                      8:1    0    1M  0 part 
    ├─sda2                      8:2    0    1G  0 part /boot
    └─sda3                      8:3    0   39G  0 part 
      ├─VolGroup00-LogVol01   253:1    0  1.5G  0 lvm  [SWAP]
      └─VolGroup00-LogVol00   253:2    0 37.5G  0 lvm  
    sdb                         8:16   0   10G  0 disk 
    sdc                         8:32   0    2G  0 disk 
    sdd                         8:48   0    1G  0 disk 
    sde                         8:64   0    1G  0 disk 
    sdf                         8:80   0   40G  0 disk 
    └─vg_tmp_root-lv_tmp_root 253:0    0   40G  0 lvm  / <-- ВОТ
sudo lvremove /dev/VolGroup00/LogVol00
    Do you really want to remove active logical volume VolGroup00/LogVol00? [y/n]: y
      Logical volume "LogVol00" successfully removed
sudo lvcreate -n LogVol00 -L 8G VolGroup00
    Logical volume "LogVol00" created.
sudo mkfs.xfs /dev/VolGroup00/LogVol00
    meta-data=/dev/VolGroup00/LogVol00 isize=512    agcount=4, agsize=524288 blks
             =                       sectsz=512   attr=2, projid32bit=1
             =                       crc=1        finobt=0, sparse=0
    data     =                       bsize=4096   blocks=2097152, imaxpct=25
             =                       sunit=0      swidth=0 blks
    naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
    log      =internal log           bsize=4096   blocks=2560, version=2
             =                       sectsz=512   sunit=0 blks, lazy-count=1
    realtime =none                   extsz=4096   blocks=0, rtextents=0
sudo mount /dev/VolGroup00/LogVol00 /mnt
sudo xfsdump -J - /dev/vg_tmp_root/lv_tmp_root | sudo xfsrestore -J - /mnt
    ...
    xfsrestore: restore complete: 37 seconds elapsed
    xfsrestore: Restore Status: SUCCESS

for i in /proc/ /sys/ /dev/ /run/ /boot/; do sudo mount --bind $i /mnt/$i; done
sudo chroot /mnt/
```
Далее `[root@lvm /]`:
```shell
grub2-mkconfig -o /boot/grub2/grub.cfg
    Generating grub configuration file ...
    Found linux image: /boot/vmlinuz-3.10.0-862.2.3.el7.x86_64
    Found initrd image: /boot/initramfs-3.10.0-862.2.3.el7.x86_64.img
    done
cd /boot ; for i in `ls initramfs-*img`; do dracut -v $i `echo $i|sed "s/initramfs-//g; s/.img//g"` --force; done
    ...
    *** Creating initramfs image file '/boot/initramfs-3.10.0-862.2.3.el7.x86_64.img' done ***

cat /boot/grub2/grub.cfg | grep 'LogVol00'
    linux16 /vmlinuz-3.10.0-862.2.3.el7.x86_64 root=/dev/mapper/VolGroup00-LogVol00 ro no_timer_check console=tty0 console=ttyS0,115200n8 net.ifnames=0 biosdevname=0 elevator=noop crashkernel=auto rd.lvm.lv=VolGroup00/LogVol00 rd.lvm.lv=VolGroup00/LogVol01 rhgb quiet
sed -i 's@vg_tmp_root/lv_tmp_root@VolGroup00/LogVol00@g' /boot/grub2/grub.cfg - НЕ НУЖЕН!!!
cat /boot/grub2/grub.cfg | grep 'vg_tmp_root'
exit
```
Далее `[vagrant@lvm ~]$`:
```shell
sudo reboot
```
Далее целевая:
```shell
vagrant ssh
```
Далее `[vagrant@lvm ~]$`<a name="1"></a>:
```shell
lsblk
    NAME                      MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
    sda                         8:0    0   40G  0 disk 
    ├─sda1                      8:1    0    1M  0 part 
    ├─sda2                      8:2    0    1G  0 part /boot
    └─sda3                      8:3    0   39G  0 part 
      ├─VolGroup00-LogVol00   253:0    0    8G  0 lvm  /   <-- вернули сжатым
      ├─VolGroup00-LogVol01   253:1    0  1.5G  0 lvm  [SWAP]
    sdb                         8:16   0   10G  0 disk 
    sdc                         8:32   0    2G  0 disk 
    sdd                         8:48   0    1G  0 disk 
    sde                         8:64   0    1G  0 disk 
    sdf                         8:80   0   40G  0 disk 
    └─vg_tmp_root-lv_tmp_root 253:3    0   40G  0 lvm  
yes "y" | sudo lvremove /dev/vg_tmp_root/lv_tmp_root
    Do you really want to remove active logical volume vg_tmp_root/lv_tmp_root? [y/n]:   Logical volume "lv_tmp_root" successfully removed
sudo vgremove vg_tmp_root
   Volume group "vg_tmp_root" successfully removed
sudo pvremove /dev/sdf
  Labels on physical volume "/dev/sdf" successfully wiped.
lsblk
    NAME                    MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
    sda                       8:0    0   40G  0 disk 
    ├─sda1                    8:1    0    1M  0 part 
    ├─sda2                    8:2    0    1G  0 part /boot
    └─sda3                    8:3    0   39G  0 part 
      ├─VolGroup00-LogVol00 253:0    0    8G  0 lvm  /
      └─VolGroup00-LogVol01 253:1    0  1.5G  0 lvm  [SWAP]
    sdb                       8:16   0   10G  0 disk 
    sdc                       8:32   0    2G  0 disk 
    sdd                       8:48   0    1G  0 disk 
    sde                       8:64   0    1G  0 disk 
    sdf                       8:80   0   40G  0 disk 
```

```shell
vagrant ssh
```

### Выделить том под /home

От `[vagrant@lvm ~]`

```shell
sudo lvcreate -n LogVol_Home -L 2G /dev/VolGroup00  
    Logical volume "LogVol_Home" created.
sudo mkfs.xfs /dev/VolGroup00/LogVol_Home
    meta-data=/dev/VolGroup00/LogVol_Home isize=512    agcount=4, agsize=131072 blks
             =                       sectsz=512   attr=2, projid32bit=1
             =                       crc=1        finobt=0, sparse=0
    data     =                       bsize=4096   blocks=524288, imaxpct=25
             =                       sunit=0      swidth=0 blks
    naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
    log      =internal log           bsize=4096   blocks=2560, version=2
             =                       sectsz=512   sunit=0 blks, lazy-count=1
    realtime =none                   extsz=4096   blocks=0, rtextents=0

sudo mount /dev/VolGroup00/LogVol_Home /mnt/
sudo cp -aR /home/* /mnt/   
sudo rm -rf /home/*
sudo umount /mnt
sudo mount /dev/VolGroup00/LogVol_Home /home/
# Правим fstab для автоматического монтирования /home
sudo echo "`sudo blkid | grep Home | awk '{print $2}'` /home xfs defaults 0 0" | sudo tee -a /etc/fstab
    UUID="1a3604a9-b705-4969-8596-53ed659fd71c" /home xfs defaults 0 0
```

Этап <a name="2"></a>:

```shell
lsblk
    NAME                       MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
    sda                          8:0    0   40G  0 disk 
    ├─sda1                       8:1    0    1M  0 part 
    ├─sda2                       8:2    0    1G  0 part /boot
    └─sda3                       8:3    0   39G  0 part 
      ├─VolGroup00-LogVol00    253:0    0    8G  0 lvm  /
      ├─VolGroup00-LogVol01    253:1    0  1.5G  0 lvm  [SWAP]
      └─VolGroup00-LogVol_Home 253:3    0    2G  0 lvm  /home <--- ВОТ
    sdb                          8:16   0   10G  0 disk 
    sdc                          8:32   0    2G  0 disk 
    sdd                          8:48   0    1G  0 disk 
    sde                          8:64   0    1G  0 disk 
    sdf                          8:80   0   40G  0 disk 
```

```shell
cat /etc/fstab
    #
    # /etc/fstab
    # Created by anaconda on Sat May 12 18:50:26 2018
    #
    # Accessible filesystems, by reference, are maintained under '/dev/disk'
    # See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info
    #
    /dev/mapper/VolGroup00-LogVol00 /                       xfs     defaults        0 0
    UUID=570897ca-e759-4c81-90cf-389da6eee4cc /boot         xfs     defaults        0 0
    /dev/mapper/VolGroup00-LogVol01 swap                    swap    defaults        0 0
    #VAGRANT-BEGIN
    # The contents below are automatically generated by Vagrant. Do not modify.
    #VAGRANT-END
    UUID="6ae19260-bac9-464a-9520-4f91264acfd4" /home xfs defaults 0 0
    UUID="22e23d1a-31c0-4dd9-8628-1881a0425362" /var  xfs defaults 0 0
```
### Выделить том под /var (+ сразу учтем зеркалирование)

Сделаем на другом диске. От `[vagrant@lvm ~]$`
```shell
sudo pvcreate /dev/sdd /dev/sde
    Physical volume "/dev/sdd" successfully created.
    Physical volume "/dev/sde" successfully created.
sudo vgcreate VolGroupVar /dev/sdd /dev/sde
    Volume group "VolGroupVar" successfully created
sudo vgdisplay -v VolGroupVar
    --- Volume group ---
    VG Name               VolGroupVar
    System ID             
    Format                lvm2
    Metadata Areas        2
    Metadata Sequence No  1
    VG Access             read/write
    VG Status             resizable
    MAX LV                0
    Cur LV                0
    Open LV               0
    Max PV                0
    Cur PV                2
    Act PV                2
    VG Size               1.99 GiB
    PE Size               4.00 MiB
    Total PE              510
    Alloc PE / Size       0 / 0   
    Free  PE / Size       510 / 1.99 GiB
    VG UUID               xQ8MNf-uqQ2-dgyr-L0Fs-Inti-RSfb-q10Ta2
    
    --- Physical volumes ---
    PV Name               /dev/sdd     
    PV UUID               yARGcC-Azaf-fzsE-BBBP-DrTJ-z2vH-vsXS4e
    PV Status             allocatable
    Total PE / Free PE    255 / 255
    
    PV Name               /dev/sde     
    PV UUID               tdGahE-N8di-92x2-LoRv-Ovfp-pkcZ-SFPrKc
    PV Status             allocatable
    Total PE / Free PE    255 / 255
sudo lvcreate -L 950M -m1 -n var VolGroupVar
      Rounding up size to full physical extent 952.00 MiB
      Logical volume "var" created.
sudo mkfs.ext4 /dev/VolGroupVar/var
    mke2fs 1.42.9 (28-Dec-2013)
    Filesystem label=
    OS type: Linux
    Block size=4096 (log=2)
    Fragment size=4096 (log=2)
    Stride=0 blocks, Stripe width=0 blocks
    60928 inodes, 243712 blocks
    12185 blocks (5.00%) reserved for the super user
    First data block=0
    Maximum filesystem blocks=249561088
    8 block groups
    32768 blocks per group, 32768 fragments per group
    7616 inodes per group
    Superblock backups stored on blocks: 
            32768, 98304, 163840, 229376
    
    Allocating group tables: done                            
    Writing inode tables: done                            
    Creating journal (4096 blocks): done
    Writing superblocks and filesystem accounting information: done

sudo mount /dev/VolGroupVar/var /mnt/
sudo cp -aR /var/* /mnt/

# На всякий случай сохраняем содержимое старого var (или же можно его просто удалить):
sudo mkdir /tmp/oldvar && sudo mv /var/* /tmp/oldvar  <---- ПОПРАВИТЬ

sudo umount /mnt 
sudo mount /dev/VolGroupVar/var /var
sudo echo "`sudo blkid | grep "VolGroupVar-var:" | awk '{print $2}'` /var xfs defaults 0 0" | sudo tee -a /etc/fstab 
                                                ^-вот так
    UUID="6ec94ccb-2292-43e5-b18f-1de946f045fc" /var xfs defaults 0 0
```
Этап <a name="34"></a>:

```shell
lsblk

    NAME                       MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
    sda                          8:0    0   40G  0 disk 
    ├─sda1                       8:1    0    1M  0 part 
    ├─sda2                       8:2    0    1G  0 part /boot
    └─sda3                       8:3    0   39G  0 part 
      ├─VolGroup00-LogVol00    253:0    0    8G  0 lvm  /
      ├─VolGroup00-LogVol01    253:1    0  1.5G  0 lvm  [SWAP]
      └─VolGroup00-LogVol_Home 253:2    0    2G  0 lvm  /home
    sdb                          8:16   0   10G  0 disk 
    sdc                          8:32   0    2G  0 disk 
    sdd                          8:48   0    1G  0 disk 
    ├─VolGroupVar-var_rmeta_0  253:4    0    4M  0 lvm  
    │ └─VolGroupVar-var        253:8    0  952M  0 lvm  /var <---.
    └─VolGroupVar-var_rimage_0 253:5    0  952M  0 lvm           |---.
      └─VolGroupVar-var        253:8    0  952M  0 lvm  /var <---╯   |
    sde                          8:64   0    1G  0 disk              |
    ├─VolGroupVar-var_rmeta_1  253:6    0    4M  0 lvm               | --- Mirror 1
    │ └─VolGroupVar-var        253:8    0  952M  0 lvm  /var <---.   |
    └─VolGroupVar-var_rimage_1 253:7    0  952M  0 lvm           |---╯
      └─VolGroupVar-var        253:8    0  952M  0 lvm  /var <---╯
    sdf                          8:80   0   40G  0 disk 
```

### сделать том для снапшотов /home

```shell
sudo mkdir /home/user_{1..20}
```
<a name="56"></a>

Вопрос

```shell    

sudo lvcreate -L 100MB -s -n home_snap /dev/VolGroup00/LogVol_Home
    Rounding up size to full physical extent 128.00 MiB
    Logical volume "home_snap" created.
lsblk
    NAME                            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
    sda                               8:0    0   40G  0 disk 
    ├─sda1                            8:1    0    1M  0 part 
    ├─sda2                            8:2    0    1G  0 part /boot
    └─sda3                            8:3    0   39G  0 part 
      ├─VolGroup00-LogVol00         253:0    0    8G  0 lvm  /
      ├─VolGroup00-LogVol01         253:1    0  1.5G  0 lvm  [SWAP]
      ├─VolGroup00-LogVol_Home-real 253:2    0    2G  0 lvm  
      │ ├─VolGroup00-LogVol_Home    253:3    0    2G  0 lvm  /home
      │ └─VolGroup00-home_snap      253:10   0    2G  0 lvm  
      └─VolGroup00-home_snap-cow    253:9    0  128M  0 lvm  <---- snapshot
        └─VolGroup00-home_snap      253:10   0    2G  0 lvm 
    sdb                               8:16   0   10G  0 disk 
    sdc                               8:32   0    2G  0 disk 
    sdd                               8:48   0    1G  0 disk 
    ├─VolGroupVar-var_rmeta_0       253:4    0    4M  0 lvm  
    │ └─VolGroupVar-var             253:8    0  952M  0 lvm  /var
    └─VolGroupVar-var_rimage_0      253:5    0  952M  0 lvm  
      └─VolGroupVar-var             253:8    0  952M  0 lvm  /var
    sde                               8:64   0    1G  0 disk 
    ├─VolGroupVar-var_rmeta_1       253:6    0    4M  0 lvm  
    │ └─VolGroupVar-var             253:8    0  952M  0 lvm  /var
    └─VolGroupVar-var_rimage_1      253:7    0  952M  0 lvm  
      └─VolGroupVar-var             253:8    0  952M  0 lvm  /var
    sdf                               8:80   0   40G  0 disk 

```

<a name="8"></a>
```shell
sudo rm -r /home/user_{10..20}
ls -l /home <-- данные удаленв в home
    total 0
    drwxr-xr-x. 2 root    root     6 May 10 21:02 user_1
    drwxr-xr-x. 2 root    root     6 May 10 21:02 user_2
    drwxr-xr-x. 2 root    root     6 May 10 21:02 user_3
    drwxr-xr-x. 2 root    root     6 May 10 21:02 user_4
    drwxr-xr-x. 2 root    root     6 May 10 21:02 user_5
    drwxr-xr-x. 2 root    root     6 May 10 21:02 user_6
    drwxr-xr-x. 2 root    root     6 May 10 21:02 user_7
    drwxr-xr-x. 2 root    root     6 May 10 21:02 user_8
    drwxr-xr-x. 2 root    root     6 May 10 21:02 user_9
    drwx------. 3 vagrant vagrant 95 May  9 00:27 vagrant
```
<a name="9"></a>
```shell
sudo umount /home
sudo lvconvert --merge /dev/VolGroup00/home_snap
    Merging of volume VolGroup00/home_snap started.
    VolGroup00/LogVol_Home:  100.00%
lsblk
    ...
    NAME                       MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
    sda                          8:0    0   40G  0 disk 
    ├─sda1                       8:1    0    1M  0 part 
    ├─sda2                       8:2    0    1G  0 part /boot
    └─sda3                       8:3    0   39G  0 part 
      ├─VolGroup00-LogVol00    253:0    0    8G  0 lvm  /
      ├─VolGroup00-LogVol01    253:1    0  1.5G  0 lvm  [SWAP]
      └─VolGroup00-LogVol_Home 253:3    0    2G  0 lvm  <---- snapshot применет и отсутствует
    sdb                          8:16   0   10G  0 disk 
    sdc                          8:32   0    2G  0 disk 
    sdd                          8:48   0    1G  0 disk 
    ├─VolGroupVar-var_rmeta_0  253:4    0    4M  0 lvm  
    │ └─VolGroupVar-var        253:8    0  952M  0 lvm  /var
    └─VolGroupVar-var_rimage_0 253:5    0  952M  0 lvm  
      └─VolGroupVar-var        253:8    0  952M  0 lvm  /var
    sde                          8:64   0    1G  0 disk 
    ├─VolGroupVar-var_rmeta_1  253:6    0    4M  0 lvm  
    │ └─VolGroupVar-var        253:8    0  952M  0 lvm  /var
    └─VolGroupVar-var_rimage_1 253:7    0  952M  0 lvm  
      └─VolGroupVar-var        253:8    0  952M  0 lvm  /var
    sdf                          8:80   0   40G  0 disk
sudo mount /home
ls -l /home <-- данные восстановлены в home из снимка home_snap
    total 0
    drwxr-xr-x. 2 root    root     6 May 15 19:24 user_1
    drwxr-xr-x. 2 root    root     6 May 15 19:24 user_10
    drwxr-xr-x. 2 root    root     6 May 15 19:24 user_11
    drwxr-xr-x. 2 root    root     6 May 15 19:24 user_12
    drwxr-xr-x. 2 root    root     6 May 15 19:24 user_13
    drwxr-xr-x. 2 root    root     6 May 15 19:24 user_14
    drwxr-xr-x. 2 root    root     6 May 15 19:24 user_15
    drwxr-xr-x. 2 root    root     6 May 15 19:24 user_16
    drwxr-xr-x. 2 root    root     6 May 15 19:24 user_17
    drwxr-xr-x. 2 root    root     6 May 15 19:24 user_18
    drwxr-xr-x. 2 root    root     6 May 15 19:24 user_19
    drwxr-xr-x. 2 root    root     6 May 15 19:24 user_2
    drwxr-xr-x. 2 root    root     6 May 15 19:24 user_20
    drwxr-xr-x. 2 root    root     6 May 15 19:24 user_3
    drwxr-xr-x. 2 root    root     6 May 15 19:24 user_4
    drwxr-xr-x. 2 root    root     6 May 15 19:24 user_5
    drwxr-xr-x. 2 root    root     6 May 15 19:24 user_6
    drwxr-xr-x. 2 root    root     6 May 15 19:24 user_7
    drwxr-xr-x. 2 root    root     6 May 15 19:24 user_8
    drwxr-xr-x. 2 root    root     6 May 15 19:24 user_9
    drwx------. 3 vagrant vagrant 74 May 12  2018 vagrant
```

Вопрос: после применения снепшота - раздел пропал. Так и должно быть?

<a name="10"></a>
[https://zenway.ru/page/script](https://zenway.ru/page/script)
```shell
script -f ../terminal.log
    Script started, file is ../terminal.log
    ...
script -q ../terminal.log
```
