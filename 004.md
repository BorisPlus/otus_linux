#  ZFS

__Домашнее задание__

Практические навыки работы с ZFS.

__Цель__:

Отрабатываем навыки работы с созданием томов export/import и установкой параметров.
* Определить алгоритм с наилучшим сжатием - [определен](#1): gzip-9
* Определить настройки pool’a - [определены](#2)
* Найти сообщение от преподавателей - [найдено](#3)

__Результат__: 
* список команд которыми получен результат с их выводами 
  
__1. Определить алгоритм с наилучшим сжатием__

__Зачем__: 
Отрабатываем навыки работы с созданием томов и установкой параметров. Находим наилучшее сжатие.

__Шаги__:
* определить какие алгоритмы сжатия поддерживает zfs (gzip gzip-N, zle lzjb, lz4)
* создать 4 файловых системы на каждой применить свой алгоритм сжатия.
  Для сжатия использовать либо текстовый файл, либо группу файлов:
  * либо скачать файл “Война и мир” и расположить на файловой системе 
  ```shell
    wget -O War_and_Peace.txt http://www.gutenberg.org/ebooks/2600.txt.utf-8 
  ```
  * либо скачать файл ядра распаковать и расположить на файловой системе

__Результат__:
* список команд которыми получен результат с их выводами
* вывод команды из которой видно какой из алгоритмов лучше 
  
__2. Определить настройки pool’a__

__Зачем__: 
Для переноса дисков между системами используется функция export/import. Отрабатываем навыки работы с файловой системой ZFS

__Шаги__:
* Загрузить архив с файлами локально. https://drive.google.com/open?id=1KRBNW33QWqbvbVHa3hLJivOAt60yukkg
* Распаковать.
* С помощью команды zfs import собрать pool ZFS.
* Командами zfs определить настройки
  * размер хранилища
  * тип pool
  * значение recordsize
  * какое сжатие используется
  * какая контрольная сумма используется - алгоритм?

__Результат__:
  * список команд которыми восстановили pool. Желательно с Output команд.
  * файл с описанием настроек settings

__3. Найти сообщение от преподавателей__

__Зачем__: 
* для бэкапа используются технологии snapshot. Snapshot можно передавать между хостами и восстанавливать с помощью send/receive. Отрабатываем навыки восстановления snapshot и переноса файла.

__Шаги__:
* Скопировать файл из удаленной директории. https://drive.google.com/file/d/1gH8gCL9y7Nd5Ti3IRmplZPF1XjzxeRAG/view?usp=sharing 
  Файл был получен командой zfs send otus/storage@task2 > otus_task2.file
* Восстановить файл локально. zfs receive
* Найти зашифрованное сообщение в файле secret_message

__Результат__:
* список шагов которыми восстанавливали
* зашифрованное сообщение

## Результат ДЗ

* Определить алгоритм с наилучшим сжатием - [определен](#1)
* Определить настройки pool’a - [определены](#2)
* Найти сообщение от преподавателей - [найдено](#3)

## Исполнение

[Vagrantfile](./004/src/Vagrantfile) образа. Запускаем с целевой системы:
```shell
vagrant up
vagrant ssh server
```

### 1. Определить алгоритм с наилучшим сжатием

__Внимание__: здесь и далее команды запускаются внутри VM, то есть в терминале значится метка `[vagrant@lvm ~]$`, если не сказано иное.

Текущее распределение дискового пространства:

```shell
lsblk
    NAME   MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
    sda      8:0    0  10G  0 disk 
    `-sda1   8:1    0  10G  0 part /
    sdb      8:16   0   1G  0 disk 
    sdc      8:32   0   1G  0 disk 
    sdd      8:48   0   1G  0 disk 
    sde      8:64   0   1G  0 disk 
    sdf      8:80   0   1G  0 disk 
    sdg      8:96   0   1G  0 disk 
```

Значение "сжатия":
* off 
* on 
* lzjb 
* gzip 
* gzip-[1-9] 
* zle 
* lz4

Создадим под каждый вид сжатия свой раздел:

```shell
sudo zpool create raw_drive sdb
sudo zpool create lzjb_drive sdc
sudo zpool create gzip1_drive sdd
sudo zpool create gzip9_drive sde
sudo zpool create zle_drive sdf
sudo zpool create lz4_drive sdg

zpool list
    NAME          SIZE  ALLOC   FREE  CKPOINT  EXPANDSZ   FRAG    CAP  DEDUP    HEALTH  ALTROOT
    gzip1_drive   960M  94.5K   960M        -         -     0%     0%  1.00x    ONLINE  -
    gzip9_drive   960M  94.5K   960M        -         -     0%     0%  1.00x    ONLINE  -
    lz4_drive     960M  94.5K   960M        -         -     0%     0%  1.00x    ONLINE  -
    lzjb_drive    960M  94.5K   960M        -         -     0%     0%  1.00x    ONLINE  -
    raw_drive     960M  94.5K   960M        -         -     0%     0%  1.00x    ONLINE  -
    zle_drive     960M  94.5K   960M        -         -     0%     0%  1.00x    ONLINE  -

sudo zfs set compression=off raw_drive
sudo zfs set compression=lzjb lzjb_drive
sudo zfs set compression=gzip-1 gzip1_drive
sudo zfs set compression=gzip-9 gzip9_drive
sudo zfs set compression=zle lzjb_drive
sudo zfs set compression=lz4 lz4_drive

zpool list
    NAME          SIZE  ALLOC   FREE  CKPOINT  EXPANDSZ   FRAG    CAP  DEDUP    HEALTH  ALTROOT
    gzip1_drive   960M   146K   960M        -         -     0%     0%  1.00x    ONLINE  -
    gzip9_drive   960M   141K   960M        -         -     0%     0%  1.00x    ONLINE  -
    lz4_drive     960M   141K   960M        -         -     0%     0%  1.00x    ONLINE  -
    lzjb_drive    960M   158K   960M        -         -     0%     0%  1.00x    ONLINE  -
    raw_drive     960M   120K   960M        -         -     0%     0%  1.00x    ONLINE  -
    zle_drive     960M  94.5K   960M        -         -     0%     0%  1.00x    ONLINE  -
                          ^
                  !!!изменилось!!!
```

__Замечание__: занимаемое в разделах пространство немного повысилось.

```shell
pwd
    /home/vagrant

curl -J -L https://www.kernel.org/pub/linux/kernel/v3.x/linux-3.12.9.tar.xz --output linux-3.12.9.tar.xz
tar -xf linux-3.12.9.tar.xz

sudo cp -r ./linux-3.12.9 /raw_drive
sudo cp -r ./linux-3.12.9 /gzip1_drive
sudo cp -r ./linux-3.12.9 /gzip9_drive
sudo cp -r ./linux-3.12.9 /lz4_drive
sudo cp -r ./linux-3.12.9 /lzjb_drive
sudo cp -r ./linux-3.12.9 /zle_drive

du -s /raw_drive   | awk '{print $1}' 557921 - образец
du -s /gzip1_drive | awk '{print $1}' 193264 - сжало в 2.89 раза
du -s /gzip9_drive | awk '{print $1}' 171251 - сжало в 3.26 раза
du -s /lz4_drive   | awk '{print $1}' 250890 - сжало в 2.22 раза
du -s /lzjb_drive  | awk '{print $1}' 535367 - сжало в 1.04 раза
du -s /zle_drive   | awk '{print $1}' 557920 - сжало в 1.000002 раза
```

<a name="1"></a>

На использованных мною данных максимальную степень сжатия продемонстировал GZip, а именно `gzip-9`.

### 2. Определить настройки pool’a 

__Замечание__: для документа https://drive.google.com/open?id=1KRBNW33QWqbvbVHa3hLJivOAt60yukkg корректная ссылка для скачивания https://docs.google.com/uc?export=download&id=1KRBNW33QWqbvbVHa3hLJivOAt60yukkg 

```shell
curl -L -J 'https://docs.google.com/uc?export=download&id=1KRBNW33QWqbvbVHa3hLJivOAt60yukkg' --output zfs_task1.tar
      % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                     Dload  Upload   Total   Spent    Left  Speed
    100   388    0   388    0     0     81      0 --:--:--  0:00:04 --:--:--    81
    100 7104k    0 7104k    0     0   561k      0 --:--:--  0:00:12 --:--:-- 1001k

tar -xf zfs_task1.tar

ls
    zfs_task1.tar  zpoolexport

ls -la zpoolexport
    total 1024000
    drwxr-xr-x. 2 vagrant vagrant        32 May 15  2020 .
    drwx------. 4 vagrant vagrant       114 May 16 21:55 ..
    -rw-r--r--. 1 vagrant vagrant 524288000 May 15  2020 filea
    -rw-r--r--. 1 vagrant vagrant 524288000 May 15  2020 fileb

sudo zpool import -d filea
     pool: otus  <--- [имя запомним, пригодится]
       id: 6554193320433390805 <--- [проще имя запомнить]
    state: DEGRADED
   status: One or more devices are missing from the system.
   action: The pool can be imported despite missing or damaged devices.  The
          fault tolerance of the pool may be compromised if imported.
     see: http://zfsonlinux.org/msg/ZFS-8000-2Q
   config:
          otus                                 DEGRADED
            mirror-0                           DEGRADED  <--- [в результат]
              /home/vagrant/zpoolexport/filea  ONLINE
              /root/zpoolexport/fileb          UNAVAIL  cannot open
              
sudo zpool import -d fileb
     pool: otus
       id: 6554193320433390805
    state: DEGRADED
   status: One or more devices are missing from the system.
   action: The pool can be imported despite missing or damaged devices.  The
          fault tolerance of the pool may be compromised if imported.
     see: http://zfsonlinux.org/msg/ZFS-8000-2Q
   config:
          otus                                 DEGRADED
            mirror-0                           DEGRADED
              /root/zpoolexport/filea          UNAVAIL  cannot open
              /home/vagrant/zpoolexport/fileb  ONLINE

sudo zpool import -d ./ otus fork_otus_pool 

zpool status 
      pool: fork_otus_pool
     state: ONLINE
      scan: none requested
    config:
            NAME                                 STATE     READ WRITE CKSUM
            fork_otus_pool                       ONLINE       0     0     0
              mirror-0                           ONLINE       0     0     0
                /home/vagrant/zpoolexport/filea  ONLINE       0     0     0
                /home/vagrant/zpoolexport/fileb  ONLINE       0     0     0
    errors: No known data errors

sudo zpool list
    NAME             SIZE  ALLOC   FREE  CKPOINT  EXPANDSZ   FRAG    CAP  DEDUP    HEALTH  ALTROOT
    fork_otus_pool   480M  2.09M   478M        -         -     0%     0%  1.00x    ONLINE  -
                      ^
                [в результат]
  
sudo zfs get all fork_otus_pool 
    NAME            PROPERTY              VALUE                  SOURCE
    fork_otus_pool  type                  filesystem             -
    fork_otus_pool  creation              Fri May 15  4:00 2020  -
    fork_otus_pool  used                  2.04M                  -
    fork_otus_pool  available             350M                   -
    fork_otus_pool  referenced            24K                    -
    fork_otus_pool  compressratio         1.00x                  -
    fork_otus_pool  mounted               yes                    -
    fork_otus_pool  quota                 none                   default
    fork_otus_pool  reservation           none                   default
    fork_otus_pool  recordsize            128K                   local   <------ [в результат]
    fork_otus_pool  mountpoint            /fork_otus_pool        default 
    fork_otus_pool  sharenfs              off                    default
    fork_otus_pool  checksum              sha256                 local   <------ [в результат]
    fork_otus_pool  compression           zle                    local   <------ [в результат]
    fork_otus_pool  atime                 on                     default
    fork_otus_pool  devices               on                     default
    fork_otus_pool  exec                  on                     default
    fork_otus_pool  setuid                on                     default
    fork_otus_pool  readonly              off                    default
    fork_otus_pool  zoned                 off                    default
    fork_otus_pool  snapdir               hidden                 default
    fork_otus_pool  aclinherit            restricted             default
    fork_otus_pool  createtxg             1                      -
    fork_otus_pool  canmount              on                     default
    fork_otus_pool  xattr                 on                     default
    fork_otus_pool  copies                1                      default
    fork_otus_pool  version               5                      -
    fork_otus_pool  utf8only              off                    -
    fork_otus_pool  normalization         none                   -
    fork_otus_pool  casesensitivity       sensitive              -
    fork_otus_pool  vscan                 off                    default
    fork_otus_pool  nbmand                off                    default
    fork_otus_pool  sharesmb              off                    default
    fork_otus_pool  refquota              none                   default
    fork_otus_pool  refreservation        none                   default
    fork_otus_pool  guid                  14592242904030363272   -
    fork_otus_pool  primarycache          all                    default
    fork_otus_pool  secondarycache        all                    default
    fork_otus_pool  usedbysnapshots       0B                     -
    fork_otus_pool  usedbydataset         24K                    -
    fork_otus_pool  usedbychildren        2.01M                  -
    fork_otus_pool  usedbyrefreservation  0B                     -
    fork_otus_pool  logbias               latency                default
    fork_otus_pool  objsetid              54                     -
    fork_otus_pool  dedup                 off                    default
    fork_otus_pool  mlslabel              none                   default
    fork_otus_pool  sync                  standard               default
    fork_otus_pool  dnodesize             legacy                 default
    fork_otus_pool  refcompressratio      1.00x                  -
    fork_otus_pool  written               24K                    -
    fork_otus_pool  logicalused           1020K                  -
    fork_otus_pool  logicalreferenced     12K                    -
    fork_otus_pool  volmode               default                default
    fork_otus_pool  filesystem_limit      none                   default
    fork_otus_pool  snapshot_limit        none                   default
    fork_otus_pool  filesystem_count      none                   default
    fork_otus_pool  snapshot_count        none                   default
    fork_otus_pool  snapdev               hidden                 default
    fork_otus_pool  acltype               off                    default
    fork_otus_pool  context               none                   default
    fork_otus_pool  fscontext             none                   default
    fork_otus_pool  defcontext            none                   default
    fork_otus_pool  rootcontext           none                   default
    fork_otus_pool  relatime              off                    default
    fork_otus_pool  redundant_metadata    all                    default
    fork_otus_pool  overlay               off                    default
    fork_otus_pool  encryption            off                    default
    fork_otus_pool  keylocation           none                   default
    fork_otus_pool  keyformat             none                   default
    fork_otus_pool  pbkdf2iters           0                      default
    fork_otus_pool  special_small_blocks  0                      default

zfs get compression /fork_otus_pool
    NAME            PROPERTY     VALUE     SOURCE
    fork_otus_pool  compression  zle       local  <--- [в результат], было ранее
```

<a name="2"></a>
Установлены:
  * размер хранилища = 480M
  * тип pool = mirror-0 
  * значение recordsize = 128K
  * какое сжатие используется = zle
  * какая контрольная сумма используется = sha256
  * файл с описанием настроек settings = вывод `sudo zfs get all fork_otus_pool`


### 3. Найти сообщение от преподавателей

__Замечание__: для cUrl необходим иной формат Url

```shell
curl -L -J 'https://docs.google.com/uc?export=download&id=1gH8gCL9y7Nd5Ti3IRmplZPF1XjzxeRAG' --output otus_task2.file
      % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                     Dload  Upload   Total   Spent    Left  Speed
    100   388    0   388    0     0    261      0 --:--:--  0:00:01 --:--:--   260
    100 5305k    0 5305k    0     0   371k      0 --:--:--  0:00:14 --:--:--  289k

sudo zpool create otus_task2 sdd sde

sudo zfs receive -F otus_task2 < otus_task2.file

zpool list
    NAME         SIZE  ALLOC   FREE  CKPOINT  EXPANDSZ   FRAG    CAP  DEDUP    HEALTH  ALTROOT
    otus_task2  1.88G  3.92M  1.87G        -         -     0%     0%  1.00x    ONLINE  -

ls -la /otus_task2/
    total 3473
    drwxr-xr-x.  3 root    root         11 May 15  2020 .
    dr-xr-xr-x. 19 root    root        273 May 18 20:34 ..
    -rw-r--r--.  1 root    root          0 May 15  2020 10M.file
    -rw-r--r--.  1 root    root     309987 May 15  2020 Limbo.txt
    -rw-r--r--.  1 root    root     509836 May 15  2020 Moby_Dick.txt
    -rw-r--r--.  1 root    root    1209374 May  6  2016 War_and_Peace.txt
    -rw-r--r--.  1 root    root     727040 May 15  2020 cinderella.tar
    -rw-r--r--.  1 root    root         65 May 15  2020 for_examaple.txt
    -rw-r--r--.  1 root    root          0 May 15  2020 homework4.txt
    drwxr-xr-x.  3 vagrant vagrant       4 Dec 18  2017 task1
    -rw-r--r--.  1 root    root     398635 May 15  2020 world.sql

find /otus_task2/ -name secret_message
    /otus_task2/task1/file_mess/secret_message
    
cat /otus_task2/task1/file_mess/secret_message
    https://github.com/sindresorhus/awesome
```

<a name="3"></a>
Зашифрованное сообщение - это ссылка https://github.com/sindresorhus/awesome

## Вопросы по ДЗ. Почему?

### Увеличился размер занимаемого "фактически пустого" пространства при изменении параметра сжатия
См. п.1:

Создадим под каждый вид сжатия свой раздел:

```shell
sudo zpool create raw_drive sdb
sudo zpool create lzjb_drive sdc
sudo zpool create gzip1_drive sdd
sudo zpool create gzip9_drive sde
sudo zpool create zle_drive sdf
sudo zpool create lz4_drive sdg

zpool list
    NAME          SIZE  ALLOC   FREE  CKPOINT  EXPANDSZ   FRAG    CAP  DEDUP    HEALTH  ALTROOT
    gzip1_drive   960M  94.5K   960M        -         -     0%     0%  1.00x    ONLINE  -
    gzip9_drive   960M  94.5K   960M        -         -     0%     0%  1.00x    ONLINE  -
    lz4_drive     960M  94.5K   960M        -         -     0%     0%  1.00x    ONLINE  -
    lzjb_drive    960M  94.5K   960M        -         -     0%     0%  1.00x    ONLINE  -
    raw_drive     960M  94.5K   960M        -         -     0%     0%  1.00x    ONLINE  -
    zle_drive     960M  94.5K   960M        -         -     0%     0%  1.00x    ONLINE  -

sudo zfs set compression=off raw_drive
sudo zfs set compression=lzjb lzjb_drive
sudo zfs set compression=gzip-1 gzip1_drive
sudo zfs set compression=gzip-9 gzip9_drive
sudo zfs set compression=zle lzjb_drive
sudo zfs set compression=lz4 lz4_drive

zpool list
    NAME          SIZE  ALLOC   FREE  CKPOINT  EXPANDSZ   FRAG    CAP  DEDUP    HEALTH  ALTROOT
    gzip1_drive   960M   146K   960M        -         -     0%     0%  1.00x    ONLINE  -
    gzip9_drive   960M   141K   960M        -         -     0%     0%  1.00x    ONLINE  -
    lz4_drive     960M   141K   960M        -         -     0%     0%  1.00x    ONLINE  -
    lzjb_drive    960M   158K   960M        -         -     0%     0%  1.00x    ONLINE  -
    raw_drive     960M   120K   960M        -         -     0%     0%  1.00x    ONLINE  -
    zle_drive     960M  94.5K   960M        -         -     0%     0%  1.00x    ONLINE  -
                          ^
                  !!!изменилось!!!
```

Занимаемое в разделах пространство немного повысилось - при этом диски еще фактически не содержат данных. Видимо в самих разделах хранится информация о применяемом на нем сжатии. Интересно, что для `lzjb` повысилось больше всех, но в результате степень сжатия у него средняя и не превосходит `gzip`.


### Степень сжатия рассчитанная мной отличается от представленной командой

См. п.1:

```shell
du -s /raw_drive   | awk '{print $1}' 557921 - образец
du -s /gzip1_drive | awk '{print $1}' 193264 - сжало в 2.89 раза
du -s /gzip9_drive | awk '{print $1}' 171251 - сжало в 3.26 раза
du -s /lz4_drive   | awk '{print $1}' 250890 - сжало в 2.22 раза
du -s /lzjb_drive  | awk '{print $1}' 535367 - сжало в 1.04 раза
du -s /zle_drive   | awk '{print $1}' 557920 - сжало в 1.000002 раза
```

Но почему разнИтся с этим? 

```shell
zfs get compressratio
    NAME         PROPERTY       VALUE  SOURCE
    gzip1_drive  compressratio  3.18x  -
    gzip9_drive  compressratio  3.64x  -
    lz4_drive    compressratio  2.40x  -
    lzjb_drive   compressratio  1.08x  -
    raw_drive    compressratio  1.00x  -
    zle_drive    compressratio  1.00x  -
```

Хотя порядок следования типов сжатия сохранен: `gzip-9`, `gzip-1`, `lz4`, `lzjb`, `zle`. При этом `zle` = `raw` (`compression=off`)

### Почему `zle` оказался не эффективным

Из п.1:

```shell
du -s /raw_drive   | awk '{print $1}' 557921 - образец
du -s /zle_drive   | awk '{print $1}' 557920 - сжало в 1.000002 раза
```

```shell
zfs get compressratio
    NAME         PROPERTY       VALUE  SOURCE
    raw_drive    compressratio  1.00x  -
    zle_drive    compressratio  1.00x  -
```
Фактически `zle` === `raw` (`compression=off`).

Почему `zle` не эффективен в моем случае (текстовые файлы)? В каком варианте и какое сжатие лучше применять?