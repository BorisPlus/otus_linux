#  VPN

Между двумя виртуалками поднять vpn в режимах
* tun;
* tap; 
* Прочуствовать разницу.

Поднять RAS на базе OpenVPN с клиентскими сертификатами, подключиться с локальной машины на виртуалку.

3*. Самостоятельно изучить, поднять ocserv и подключиться с хоста к виртуалке

## Исполнение

<a name='regards'></a> Выражаю благодарность однокурсникам lupus (Юрий) и Сергею Артемову за помощь в освоении данного материала и настройке виртуального окружения.

Я добился того, что:
* стенд разворачивается автоматом с нужными конфигами, генерацией ключей (кроме их подписывания - об это ниже)
* имеется проверка на работоспособность:
  * статус `systemctl ...`
  * лог `/var/log/...`
  * интерфейсы

Текущая настройка идет в `tun`, считая что для связи между хоставми необходима маршрутизация, то есть сетефой уровень связности (L3).
В `tap` возможно объединить только при наличии канальной связности устройств (L2).

### Разворачивание сервера

Для генерации ключей используется утилита EasyRSA.

```shell
cd 031/vm/
cd ../vm/
vagrant destroy -f && vagrant up 
python3 v2a.py -o ../ansible/inventories/hosts # Это уже как кредо
cd ../ansible/
```

```shell
ansible-playbook playbooks/generate_vpn_server_certs.yml > ../files/generate_vpn_server_certs.yml.txt
```

Обратите внимание на итоговое предупреждение `playbooks/generate_vpn_server_certs.yml`.

[details --no-link]:[лог `playbooks/generate_vpn_server_certs.yml`](./031/files/generate_vpn_server_certs.yml.txt)

К сожалению, команда `sign-req` не автоматизируема, что связано с невозможностью осуществления автоматического ввода ответа на вопросы утилиты по факту ключевой фразы к корневому сертификату центра авторизации

```shell
cd ../vm
vagrant ssh server
```

Это внутри виртуалки `server`:

```shell
cd /home/vagrant/common/temp/easyrsa/ 
./easyrsa sign-req server server # <--- вот тут вводим неавтоматизируемые ответы

exit
```

__Замечание__: так как OpenVPN сервер работать будет так же и на не стандартных портах, то в политику SELinux необходимо внести коррективы  (все уже реализовано в плейбуке, но лучше проверить вручную, у меня порты были свободны).
```shell
semanage port -l | grep 1194
  openvpn_port_t                 tcp      1194
  openvpn_port_t                 udp      1194

semanage port -l | grep 1195
semanage port -l | grep 1196

semanage port -a -t openvpn_port_t -p udp 1195 
semanage port -a -t openvpn_port_t -p udp 1196
```

Продолжаем разворачивание инфрастуктуры.

```shell
cd ../ansible
ansible-playbook playbooks/configure_openvpn_server.yml > ../files/configure_openvpn_server.yml.txt
```

[details --no-link]:[лог `playbooks/configure_openvpn_server.yml`](./031/files/configure_openvpn_server.yml.txt)

### Демонстрация работоспособности сервера

```shell
ansible-playbook playbooks/check_openvpn_server.yml  > ../files/check_openvpn_server.yml.txt
```

[details --no-link]:[лог `playbooks/check_openvpn_server.yml`](./031/files/check_openvpn_server.yml.txt)

`/sbin/ip addr`

[details --no-link]:[содержание в проекции на tun, tap `ip addr`](./031/files/server-_sbin_ip-addr-|-awk--v-RS='(^|)[0-9]+:-'-'_^(tun|tap)[0-9]*:_-{print}')


TUN:

[details --no-link]:[статус `systemctl status openvpn@server-tun.service`](./031/files/server-systemctl-status-openvpn@server-tun.service.txt)

[details --no-link]:[лог `sudo cat /var/log/openvpn_openvpn-tun.log`](./031/files/server-sudo-cat-_var_log_openvpn_openvpn-tun.log.txt)

[details --no-link]:[лог `sudo cat /var/log/openvpn_openvpn-tun-status.log`](./031/files/server-sudo-cat-_var_log_openvpn_openvpn-tun-status.log.txt)

TAP:

[details --no-link]:[статус `systemctl status openvpn@server-tap.service`](./031/files/server-systemctl-status-openvpn@server-tap.service.txt)

[details --no-link]:[лог `sudo cat /var/log/openvpn_openvpn-tap.log`](./031/files/server-sudo-cat-_var_log_openvpn_openvpn-tap.log.txt)

[details --no-link]:[лог `sudo cat /var/log/openvpn_openvpn-tap-status.log`](./031/files/server-sudo-cat-_var_log_openvpn_openvpn-tap-status.log.txt)

TUN-RAS:

[details --no-link]:[статус `systemctl status openvpn@server-tun-ras.service`](./031/files/server-systemctl-status-openvpn@server-tun-ras.service.txt)

[details --no-link]:[лог `sudo cat /var/log/openvpn_openvpn-tun-ras.log`](./031/files/server-sudo-cat-_var_log_openvpn_openvpn-tun-ras.log.txt)

[details --no-link]:[лог `sudo cat /var/log/openvpn_openvpn-tun-ras-status.log`](./031/files/server-sudo-cat-_var_log_openvpn_openvpn-tun-ras-status.log.txt)

### Клиент

```shell
ansible-playbook playbooks/generate_vpn_client_certs.yml  > ../files/generate_vpn_client_certs.yml.txt
```

[details --no-link]:[лог `playbooks/generate_vpn_client_certs.yml`](./031/files/generate_vpn_client_certs.yml.txt)

Опять, так как `sign-req` не автоматизируема:

```shell
cd ../vm
vagrant ssh server
```

Это внутри виртуалки `server`:

```shell
cd /home/vagrant/common/temp/easyrsa/ 
./easyrsa sign-req client client # <--- вот тут вводим не автоматизируемые ответы
```

```shell
exit
cd ../ansible
```

Конфигурируем клиента TUN:

```shell
ansible-playbook playbooks/configure_openvpn_clienttun.yml > ../files/configure_openvpn_clienttun.yml.txt
```

[details --no-link]:[лог `playbooks/configure_openvpn_clienttun.yml`](./031/files/configure_openvpn_clienttun.yml.txt)

Конфигурируем клиента TAP:

```shell
ansible-playbook playbooks/configure_openvpn_clienttap.yml > ../files/configure_openvpn_clienttap.yml.txt
```

[details --no-link]:[лог `playbooks/configure_openvpn_clienttap.yml`](./031/files/configure_openvpn_clienttap.yml.txt)

Собираем в кучу сертификаты для клиента TUN-RAS:

```shell
ansible-playbook playbooks/configure_openvpn_clienttunras.yml > ../files/configure_openvpn_clienttunras.yml.txt
```

[details --no-link]:[лог `playbooks/configure_openvpn_clienttunras.yml`](./031/files/configure_openvpn_clienttunras.yml.txt)

```text
Ready. Use cert files for access to vpn 192.168.33.2 from your desktop client
```

### Демонстрация работоспособности клиента

```shell
ansible-playbook playbooks/check_openvpn_client.yml  > ../files/check_openvpn_client.yml.txt
```

[details --no-link]:[лог `playbooks/check_openvpn_client.yml`](./031/files/check_openvpn_client.yml.txt)


Проверка клиента TUN:

[details --no-link]:[лог в проекции на tun `ip addr`](./031/files/clienttun-_sbin_ip-addr-|-awk--v-RS='(^|)[0-9]+:-'-'_^(tun|tap)[0-9]*:_-{print}'.txt)


[details --no-link]:[лог `systemctl status openvpn@client.service.txt`](./031/files/clienttun-systemctl-status-openvpn@client.service.txt)

Проверка клиента TAP:

[details --no-link]:[лог `systemctl status openvpn@client.service.txt`](./031/files/clienttap-systemctl-status-openvpn@client.service.txt)

Проверка клиента TUN-RAS:

Для демонстрации примера "удаленного" доступа к ресурсам VPN, не видимых из "внешнего" мира

```shell
ansible-playbook playbooks/ras_additional.yml > ../files/ras_additional.yml.txt
```

[details --no-link]:[лог `ansible-playbook playbooks/ras_additional.yml`](./031/files/ras_additional.yml.txt)

Проверка локальной работы веб-сервера

```shell
cd ../vm
vagrant ssh server

[vagrant@server ~]$ curl 127.0.0.1:80
curl: (7) Failed connect to 127.0.0.1:80; В соединении отказано

[vagrant@server ~]$ curl 192.168.33.2:80
curl: (7) Failed connect to 192.168.33.2:80; В соединении отказано

[vagrant@server ~]$ curl 172.16.33.1:80
Hello from RAS :) # <--- Работает на сервере исключительно внутри VPN

exit
```

Теперь можно произвести подключение внешнего клиента к VPN серверу (на практике далеко в облаке с пробросом единственного порта):

```shell
sudo openvpn --config /etc/openvpn/ras_client.conf
```

С такими параметрами 

[details --no-link]:[содержание `ras_client.conf`](./031/files/ras_client.conf)


Получив "удаленный" доступ исключительно из-под VPN сети к серверу:

<img src="./031/files/img/tunras5.png">


К сожалению "тумблер" Network Manager имеет своеобразные нюансы и не дает до конца сделать такой доступ, хотя VPN соединение поднимается и стабильно

<img src="./031/files/img/tunras1.png" width="500px">

<img src="./031/files/img/tunras2.png" width="500px">

<img src="./031/files/img/tunras3.png" width="500px">

<img src="./031/files/img/tunras4.png" width="200px">


## В копилку минусов Network Manager 

Сравните, как выглядят маршруты при одной и той же конфигурации VPN-сервера. 

Первое, то, что по-умолчанию на хосте:

```shell
ip r

default via 192.168.101.1 dev enp4s0 proto dhcp metric 100 
10.0.1.0/24 dev br-9707410ef63e proto kernel scope link src 10.0.1.1 linkdown 
169.254.0.0/16 dev br-1c50456e325d scope link metric 1000 linkdown 
172.16.1.0/24 dev br-1c50456e325d proto kernel scope link src 172.16.1.1 linkdown 
172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown 
172.22.0.0/16 dev br-3ee911ac2bd5 proto kernel scope link src 172.22.0.1 linkdown 
192.168.33.0/24 dev vboxnet6 proto kernel scope link src 192.168.33.1 
192.168.101.0/24 dev enp4s0 proto kernel scope link src 192.168.101.3 metric 100 
```

Второе, то, что при консольном запуске `sudo openvpn --config /etc/openvpn/ras_client.conf`

```shell
ip r

  default via 192.168.101.1 dev enp4s0 proto dhcp metric 100 
  10.0.1.0/24 dev br-9707410ef63e proto kernel scope link src 10.0.1.1 linkdown 
  169.254.0.0/16 dev br-1c50456e325d scope link metric 1000 linkdown 
  172.16.1.0/24 dev br-1c50456e325d proto kernel scope link src 172.16.1.1 linkdown 
  172.16.33.0/24 dev tun0 proto kernel scope link src 172.16.33.2 
  172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown 
  172.22.0.0/16 dev br-3ee911ac2bd5 proto kernel scope link src 172.22.0.1 linkdown 
  192.168.33.0/24 dev vboxnet6 proto kernel scope link src 192.168.33.1 
  192.168.101.0/24 dev enp4s0 proto kernel scope link src 192.168.101.3 metric 100 

curl 172.16.33.1

  Hello from RAS :) <---- отработает отлично
```

A третье, то, что делает Network Manager при базовой конфигурации VPN клиента. И оно НЕ работает. 

```shell
ip r

  default dev tun0 proto static scope link metric 50 
  default via 192.168.101.1 dev enp4s0 proto dhcp metric 100 
  10.0.1.0/24 dev br-9707410ef63e proto kernel scope link src 10.0.1.1 linkdown 
  169.254.0.0/16 dev br-1c50456e325d scope link metric 1000 linkdown 
  172.16.1.0/24 dev br-1c50456e325d proto kernel scope link src 172.16.1.1 linkdown 
  172.16.33.0/24 dev tun0 proto kernel scope link src 172.16.33.2 metric 50 
  172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown 
  172.22.0.0/16 dev br-3ee911ac2bd5 proto kernel scope link src 172.22.0.1 linkdown 
  192.168.33.0/24 dev vboxnet6 proto kernel scope link src 192.168.33.1 
  192.168.33.2 via 192.168.101.1 dev enp4s0 proto static metric 100 
  192.168.101.0/24 dev enp4s0 proto kernel scope link src 192.168.101.3 metric 100 
  192.168.101.1 dev enp4s0 proto static scope link metric 100 

curl 172.16.33.1 <---- НЕ отработает от слова "совсем"
```

Если кто знает, как просто поправить "тумблер" Network Manager, прошу коммит. 