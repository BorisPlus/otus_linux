# NFS, FUSE 

__Домашнее задание__:

`vagrant up` должен поднимать 2 виртуалки: сервер и клиент, на сервер должна быть расшарена директория на клиента она должна автоматически монтироваться при старте (fstab или autofs) в шаре должна быть папка upload с правами на запись

__Требования__:
* NFSv3 по UDP, включенный firewall
* Настроить аутентификацию через KERBEROS (NFSv4)

## Результата

В действительности я потратил на ДЗ 5 дней (бессонных ночей) только потому, что никоим образом не происходило автоматическое монтирование NFS-директории на NFS-клиенте после его перезагрузки. Я перепробовал множество вариантов, но использовать fstab я категорически не хотел ввиду рекомендаций, полученных в рамках лекции о порядке срабатывания: .->., `fstab`, .->., `network`, .->. Таким образом, монтирование должно быть после `network`. Было решено использовать `systemd`, так как там возможно указать эту зависимость. Однако это привело к необходимости "двойного" указания монтирования - точнее к файлу `.automount`. Но теперь гарантированно сработает после `reboot` - [как клиента так и сервера](#9).

К тому же RPC я не реализовывал, так как в ДЗ не говорилось. Это дело техники.

## Исполнение 

Шаблон [Vagrant-файла](./005/vm_2/Vagrantfile) файла:
```shell
# -*- mode: ruby -*-
# vi: set ft=ruby :

Vagrant.configure(2) do |config|
  config.vm.box = "centos/7"

  config.vm.provider "virtualbox" do |v|
    v.memory = 256
    v.cpus = 1
  end

  config.vm.define "nfs_server" do |nfs_server|
    nfs_server.vm.network "private_network", ip: "192.168.50.10", virtualbox__intnet: "net1"
    nfs_server.vm.hostname = "nfs_server"
    nfs_server.vm.provision "shell", path: "nfs_server_script.sh"
  end

  config.vm.define "nfs_client" do |nfs_client|
    nfs_client.vm.network "private_network", ip: "192.168.50.11", virtualbox__intnet: "net1"
    nfs_client.vm.hostname = "nfs_client"
    nfs_client.vm.provision "shell", path: "nfs_client_script.sh"
  end

end
```

Вызов

```shell
vagrant up
```

Поднимет две виртуалки - `nfs_server` и `nfs_client`.

### Ноды

Серверная и клиентская части могут быть реализованы указанными ниже командами.

Список команд для каждой скомпанованы в соответствующие shell-файлы для Vagrant-файла, но с некоторыми поправками. При поднятии виртуалок из коробки в них формируются шара на сервере и доступ к ней с клиента (клиент можно перезапускать). 

#### Серверная часть 

Скрипт реализации серверной части NFS.3 - [nfs_server_script.sh](./005/vm_2/nfs_server_script.sh) файла (в исходнике комментарии отсутствуют):

```shell
#!/bin/bash
sudo su
# Отключил обновление системы для скорости проверки, но мое мнение - на боевой надо включить
# yum update -y
yum -y install nfs-utils
systemctl enable nfs-server
systemctl start nfs-server
mkdir /nfs_share
chmod 0755 /nfs_share
chown root:root /nfs_share
echo 'Check read only permission for NFD-client.' | tee /nfs_share/README.txt
chown root:root /nfs_share/README.txt
mkdir /nfs_share/upload
chown -R nfsnobody:nfsnobody /nfs_share/upload
chmod 0755 -R /nfs_share/upload
# Заметьте - есть правила именования файлов точек доступа:
#   1. суффикс - *.exports
echo '/nfs_share 192.168.50.11(rw,sync,root_squash,all_squash)' | tee /etc/exports.d/nfs_share.exports
# Только чтобы проверить создание доступа к /nfs_share
# exportfs -v
exportfs -r
exportfs -a
# Только чтобы проверить создание доступа к /nfs_share
# exportfs -v
systemctl restart nfs-server

yum -y install firewalld
systemctl enable firewalld
systemctl start firewalld
firewall-cmd --permanent --add-port=111/udp
firewall-cmd --permanent --add-port=111/udp
firewall-cmd --permanent --zone=public --add-service=nfs
firewall-cmd --permanent --zone=public --add-service=mountd
systemctl restart firewalld
```

#### Клиентская часть 

Скрипт реализации клиентской части NFS.3 - [nfs_client_script.sh](./005/vm_2/nfs_client_script.sh) файла (в исходнике комментарии отсутствуют):

```shell
#!/bin/bash
sudo su
# Отключил обновление системы для скорости проверки, но мое мнение - на боевой надо включить
# yum update -y
yum install -y nfs-utils
# Куда монтируем на клиенте
mkdir /mnt/nfs_share
# В качестве проверки - должно быть пусто
# ls /mnt/nfs_share
# В качестве проверки - должно примонтироваться
# mount -t nfs 192.168.50.10:/nfs_share/ /mnt/nfs_share/  -o rw,noatime,noauto,x-systemd.automount,noexec,nosuid,proto=udp,vers=3
# umount /mnt/nfs_share
# Монтировать будем через systemd
# Заметьте - есть правила именования точек монтирования:
#   1. суффикс - *.mount
#   2. преобразование имен - mnt-nfs_share преобразуется в требуемое /mnt/nfs_share
#   3. значение Where должно совпасть с именем файла точки монтирования - mnt-nfs_share

echo -e '\n[Unit]' \
        '\nDescription="NFS share automount"' \
        '\nRequires=network-online.target' \        <--- необходимая зависимость
        '\nAfter=network-online.service' \          <--- необходимая зависимость
        '\n[Mount]' \
        '\nWhat=192.168.50.10:/nfs_share/' \
        '\nWhere=mnt-nfs_share' \
        '\nType=nfs' \
        '\nDirectoryMode=0755' \
        '\nOptions=rw,noatime,noauto,x-systemd.automount,noexec,nosuid,proto=udp,vers=3' \
        '\n[Install]' \
        '\nWantedBy=multi-user.target' \
        '\n' | tee /etc/systemd/system/mnt-nfs_share.mount
echo -e '\n[Unit]' \
        '\nDescription="NFS share mount"' \
        '\nRequires=network-online.target' \        <--- необходимая зависимость
        '\nAfter=network-online.service' \          <--- необходимая зависимость
        '\n[Automount]' \                           <--- Automount того,
        '\nWhere=/mnt/nfs_share' \                  <--- что Mount выше
        '\nTimeoutIdleSec=10' \
        '\n[Install]' \
        '\nWantedBy=multi-user.target' \
        '\n' | tee /etc/systemd/system/mnt-nfs_share.automount
systemctl enable mnt-nfs_share.automount
systemctl start mnt-nfs_share.automount
systemctl restart mnt-nfs_share.mount
systemctl status mnt-nfs_share.mount
```

## Работоспособность

<a name="9"></a>
Проверим работоспособность после перезапуска клиентской части.
После перезапуска происходит автомонтирование. Права на доступ исключительно к upload работают.

```shell
sudo reboot

...
$ vagrant ssh nfs_client
...
mount | grep nfs_share
    systemd-1 on /mnt/nfs_share type autofs (rw,relatime,fd=23,pgrp=1,timeout=10,minproto=5,maxproto=5,direct,pipe_ino=26415)
    
ls -la /mnt/nfs_share/
    total 4
    drwxr-xr-x. 3 root      root      38 May 22 20:33 .
    drwxr-xr-x. 3 root      root      23 May 22 20:35 ..
    -rw-r--r--. 1 root      root      43 May 22 20:33 README.txt <-- no write permission
    drwxr-xr-x. 2 nfsnobody nfsnobody  6 May 22 20:33 upload

# разрешение на работу только с upload-директорией
echo 'test' > /mnt/nfs_share/test.txt
    -bash: /mnt/nfs_share/test.txt: Permission denied
echo 'test' > /mnt/nfs_share/upload/test.txt
cat /mnt/nfs_share/upload/test.txt
    test
```

Проверим работоспособность после перезапуска сервера.

Если отключить и включить сервер, директория восстановится.

Выключим сервер с целевой системы:
```shell
vagrant halt nfs_server
    ==> nfs_server: Attempting graceful shutdown of VM...
```
На клиенте тем временем:
```shell
[vagrant@nfs-client ~]$ ls -la /mnt/nfs_share/       
    ls: cannot access /mnt/nfs_share/: No such device
```
Включим сервер с целевой системы:
```shell
vagrant up nfs_server
    ...
    ==> nfs_server: Machine already provisioned. Run `vagrant provision` or use the `--provision`
    ==> nfs_server: flag to force provisioning. Provisioners marked to run always will still run.

```
На клиенте тем временем:
```shell
[vagrant@nfs-client ~]$ ls -la /mnt/nfs_share/       
    total 4
    drwxr-xr-x. 3 root      root      38 May 22 20:33 .
    drwxr-xr-x. 3 root      root      23 May 22 20:35 ..
    -rw-r--r--. 1 root      root      43 May 22 20:33 README.txt
    drwxr-xr-x. 2 nfsnobody nfsnobody 22 May 22 20:37 upload

```